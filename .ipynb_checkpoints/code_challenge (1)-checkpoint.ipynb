{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "bEp3lJnrJXPM",
    "outputId": "150614d4-f74b-4993-bf91-b7f9daf64cab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b55022078777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m   \u001b[1;31m#Additional scklearn functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m   \u001b[1;31m#Perforing grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Load the capital one dataset and useful python module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import missingno as mn\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "if os.path.isfile('Capital One.csv'):\n",
    "    df = pd.read_csv('Capital One.csv') \n",
    "else:\n",
    "    url = 'https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv'\n",
    "    df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzzYie9qVmLS"
   },
   "source": [
    "Required Questions: Please answer completely all five required questions.\n",
    "\n",
    "Question 1\n",
    "\n",
    "•             Programmatically download and load into your favorite analytical tool the trip data for September 2015.\n",
    "\n",
    "•             Report how many rows and columns of data you have loaded.\n",
    "\n",
    "Question 2\n",
    "\n",
    "•             Plot a histogram of the number of the trip distance (“Trip Distance”).\n",
    "\n",
    "•             Report any structure you find and any hypotheses you have about that structure.\n",
    "\n",
    "Question 3\n",
    "\n",
    "•             Report mean and median trip distance grouped by hour of day.\n",
    "\n",
    "•             We’d like to get a rough sense of identifying trips that originate or terminate at one of the NYC area airports. Can you provide a count of how many transactions fit this criteria, the average fare, and any other interesting characteristics of these trips.\n",
    "\n",
    "Question 4\n",
    "\n",
    "•             Build a derived variable for tip as a percentage of the total fare.\n",
    "\n",
    "•             Build a predictive model for tip as a percentage of the total fare. Use as much of the data as you like (or all of it). Provide an estimate of performance using an appropriate sample, and show your work.\n",
    "\n",
    "Question 5\n",
    "\n",
    "Choose only one of these options to answer for Question 5. There is no preference as to which one you choose. Please select the question that you feel your particular skills and/or expertise are best suited to. If you answer more than one, only the first will be scored.\n",
    "\n",
    "•             Option A: Distributions\n",
    "\n",
    "•             Build a derived variable representing the average speed over the course of a trip.\n",
    "\n",
    "•             Can you perform a test to determine if the average trip speeds are materially the same in all weeks of September? If you decide they are not the same, can you form a hypothesis regarding why they differ?\n",
    "\n",
    "•             Can you build up a hypothesis of average trip speed as a function of time of day?\n",
    "\n",
    "•             Option B: Visualization\n",
    "\n",
    "•             Can you build a visualization (interactive or static) of the trip data that helps us understand intra- vs. inter-borough traffic? What story does it tell about how New Yorkers use their green taxis?\n",
    "\n",
    "•             Option C: Search\n",
    "\n",
    "•             We’re thinking about promoting ride sharing. Build a function that given point a point P, find the k trip origination points nearest P.\n",
    "\n",
    "–             For this question, point P would be a taxi ride starting location picked by us at a given LAT-LONG.\n",
    "\n",
    "–             As an extra layer of complexity, consider the time for pickups, so this could eventually be used for real time ride sharing matching.\n",
    "\n",
    "–             Please explain not only how this can be computed, but how efficient your approach is (time and space complexity)\n",
    "\n",
    "•             Option D: Anomaly Detection\n",
    "\n",
    "•             What anomalies can you find in the data? Did taxi traffic or behavior deviate from the norm on a particular day/time or in a particular location?\n",
    "\n",
    "•             Using time-series analysis, clustering, or some other method, please develop a process/methodology to identify out of the norm behavior and attempt to explain why those anomalies occurred.\n",
    "\n",
    "•             Option E: Your own curiosity!\n",
    "\n",
    "•             If the data leaps out and screams some question of you that we haven’t asked, ask it and answer it! Use this as an opportunity to highlight your special skills and philosophies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YncjkOnyUQca",
    "outputId": "0cdca6db-3a29-4779-894c-afa839ce9fef"
   },
   "outputs": [],
   "source": [
    "print('Number of rows in the taxi data is:', df.shape[0])\n",
    "print('Number of columns in the taxi data is:', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vthillQKoDHE"
   },
   "source": [
    "#Initial Cleaning\n",
    "Before we jump into any analysis, i want to make sure that there is no data errors\n",
    "\n",
    "* Trip Distance has to be greater than zero\n",
    "* Fare Amount, according to http://nymag.com/nymetro/urban/features/taxi/n_20286/  the initial fare increase from 2 USD to  2.5 USD on May 3rd but i couldn't find what year this article was released, and this data is from 2015, so i will be consevitaive and say that any value fot the Fair amount under 2 USD is a data error\n",
    "* Passenger Count, there needs to be atleast 1 passenger to initiate a trip, so any value with less than 1 will be considered data error\n",
    "* If the tip ammount more than the fare amount i will consider it as a data error\n",
    "* Later on in question 5, when i compute the trip time in minutes, there are 950 records that have data error where the pick up time is equal the drop off time, so the trip time is zero, and i replace this with the median(this will be done late in the notebook)(around 900 observations)\n",
    "* The trip duration has some weird entries with trips more than 7 hours, i did a bit of research and found that the longest ride that someone reported was 6 hours, i will assume that if a trip exceed 7 hours, it's a data error and i will replace it with the median(this will be done late in the notebook)(around 8000 observations)\n",
    "*There was some observations with long trip distance ( > 7 miles) and the trip time for them was less than 1 minute, this is considered data error and i will compute the speed and any observation > 50 mph i will consider as data eror and remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Vn_9YdH8qs98",
    "outputId": "fe445883-9d12-4564-d834-1e64f2d678f4"
   },
   "outputs": [],
   "source": [
    "print('Number of trips with distance less than zero is', df['Trip_distance'][df['Trip_distance'] < 0].count())\n",
    "print('Number of Fair Amount less than 2 USD is', df['Fare_amount'][df['Fare_amount'] < 2].count())\n",
    "print('Number of trips with passengers less than 1 is', df['Passenger_count'][df['Passenger_count'] < 1].count())\n",
    "print('Number of trips with passengers with tip amount more than fare amount is', (df['Tip_amount'][df['Tip_amount'] > df['Fare_amount']]).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9JqirCDBpgQu",
    "outputId": "0b24deaf-7957-4b78-fbc6-e8d6bf6556d1"
   },
   "outputs": [],
   "source": [
    "df = df[df['Fare_amount'] >= 2]\n",
    "df = df[df['Passenger_count'] >= 1]\n",
    "df = df[df['Tip_amount'] < df['Fare_amount']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zk-ZgGXJssQQ"
   },
   "source": [
    "Now the data is clean from data errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "XYwN002XUfDc",
    "outputId": "3359a494-e2ec-4cef-e57f-2326344ac351"
   },
   "outputs": [],
   "source": [
    "#include=['Trip_Distance', 'Fare_amount', 'Tip_amount']\n",
    "df['Trip_distance'].describe()#showing some stats on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "uOCjCOPVUjbH",
    "outputId": "1673998b-6628-40b9-c2a4-1e9699a7b687"
   },
   "outputs": [],
   "source": [
    "print(df['Trip_distance'].std())\n",
    "print(df['Trip_distance'].mean())\n",
    "print(df['Trip_distance'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "qbmmmk7TUmyH",
    "outputId": "f0ae1d0e-5cb6-4b3c-c229-ba3778f7b04a"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['Trip_distance'])\n",
    "plt.title('Distribution Of Distance From Origin To Destination')\n",
    "plt.xlabel('Distance In Miles')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mwuG3EhUqS9"
   },
   "source": [
    "Here it looks that there is alot of outliers that we can't look at the distribution correct, i will leverage the central limit theory that if we randomly sample with good sample size(i am being vague about good because it depends..) the sample is going to be normally distributed (assuming independence) then 99.7% of the data will fall within 3 standard deviation of the mean.\n",
    "\n",
    "so our assumption here that anything more than 3 standard deviation, we will consider it outlier and clean it from the data so we can see the distribution\n",
    "\n",
    "cite https://en.wikipedia.org/wiki/Central_limit_theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mbHMNeLYUsm0",
    "outputId": "f52c0307-d0f8-4c36-eb15-911dcbadc6d4"
   },
   "outputs": [],
   "source": [
    "trip_distance = df['Trip_distance'][df['Trip_distance'] < 3*df['Trip_distance'].std()]\n",
    "print('the count of trip distance with outliers is',df['Trip_distance'].count())\n",
    "print('the count of trip distance without outliers is',trip_distance.count())\n",
    "print('the number of outliers is', df['Trip_distance'].count() - trip_distance.count())\n",
    "print('the percentage of outliers is',(df['Trip_distance'].count() - trip_distance.count())/df['Trip_distance'].count()*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm9s0G0cUxWP"
   },
   "source": [
    "Now let's look at the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "vYuh4_9-Uv53",
    "outputId": "5cb7f5c2-2f4c-4109-a85c-6f6ad8f44b39"
   },
   "outputs": [],
   "source": [
    "#plotting the new data without the outliers\n",
    "plt.subplot(2,1,1)\n",
    "sns.distplot(trip_distance)\n",
    "plt.title('Distribution Of Distance From Origin To Destination')\n",
    "plt.xlabel('Distance In Miles')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0,10)\n",
    "plt.subplot(2,1,2)\n",
    "#the code for simulating a lognormal distribution is form scipy\n",
    "#https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.random.lognormal.html\n",
    "mu, sigma = 3., 1. # mean and standard deviation\n",
    "s = np.random.lognormal(mu, sigma, 100)\n",
    "#Display the histogram of the samples, along with the probability density function:\n",
    "count, bins, ignored = plt.hist(s, 100, normed=True, align='mid')\n",
    "x = np.linspace(min(bins), max(bins), 10000)\n",
    "pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "        / (x * sigma * np.sqrt(2 * np.pi)))\n",
    "plt.plot(x, pdf, linewidth=2, color='r')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiQL8PHsU5FF"
   },
   "source": [
    "we can see this distribution is right skewed, this is a better picture than the lognormal simulation i did, and i can make a hypothesis that the distribution is lognormal. It's also noticed that the mean and the median is smaller than the standard deviation.\n",
    "\n",
    "Hypothesis is that the sidtribution follows a log normal distribution and it's not random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This image was taken from the statistical sleuth book**\n",
    "\n",
    "<img src=\"data/lognormal.jpg\" alt=\"TukeyBoxplot\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiatjXWyVZlH"
   },
   "source": [
    "## Question 3\n",
    "the question was alitbit vague when it said report the mean and median trip distance grouped by the hour of the day, it didn't specify the hour of pick up or drop off? since it will give different assumptions, i will assume it's the hour of the pickup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_4-9fzhW5QU"
   },
   "source": [
    "I have a problem stripping the time, i need to have a tuple with the time stripped and then grab the hour index as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXc1YpHCVAb3"
   },
   "outputs": [],
   "source": [
    "df['hour'] = df.lpep_pickup_datetime.apply(lambda x: x[11:-6])#applying lambda functions here which is way better than for loops since it uses parallel processing\n",
    "df['day']= df.lpep_pickup_datetime.apply(lambda x: x[8:10])\n",
    "#group by hour of the day and see the stats for trip distance\n",
    "trip_mean = df['Trip_distance'].groupby(df['hour']).mean()\n",
    "trip_median = df['Trip_distance'].groupby(df['hour']).median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "uxMCKk8dCcQ3",
    "outputId": "e06ea9b3-20d9-43ab-9c8a-a21da54bb983"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.plot(trip_mean)\n",
    "plt.title('Average Trip Distance Grouped By Hour Of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Average Trip Distance In Miles')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(trip_median)\n",
    "plt.title('Median Trip Distance Grouped By Hour Of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Median Trip Distance In Miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MPNWV0BDiqL"
   },
   "source": [
    "We can see the distibution of the mean and the median is the same(the median is always less than the mean)\n",
    "We can also see that the Peak is from 5 to 7 am, maybe if we group by the day and see if it was a week day or weekend will give us more explaination about the peak hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmKilKXZHg6p"
   },
   "source": [
    "We’d like to get a rough sense of identifying trips that originate or terminate at one of the NYC area airports. Can you provide a count of how many transactions fit this criteria, the average fare, and any other interesting characteristics of these trips.\n",
    "\n",
    "The question is abot vague when it said NYC airports, since there is 3 main airport. what we can do is to take a polygon shape using shapely which does manipulation and analysis of geometric objects in the Cartesian plane.\n",
    "\n",
    "but in the data dictionary the RatecodeID is the final rate code in effect at the end of the trip. the question was flexible about originating or terminate, so i will subset the data using RatecodeID 2(JFK) and 3(Newwark).\n",
    "\n",
    "If we didn't have this information we would have to build a polygon of coordinates over each airport and a polygon of coordinates in shapely and over lap the together. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "lQBD_MTDGko7",
    "outputId": "ba83b2ce-32cf-4dc1-aa35-ae32a10f1be8"
   },
   "outputs": [],
   "source": [
    "airport_transaction = df[(df['RateCodeID'] == 2) | (df['RateCodeID'] == 3)]\n",
    "airport_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NZg7nAECKFN5",
    "outputId": "fca2c238-6729-4583-c880-ebf892ad9f2a"
   },
   "outputs": [],
   "source": [
    "print('Number of transaction that terminate at JFK & Newark is',airport_transaction.shape[0])\n",
    "print('The average fare of transactions that terminate in JFK & Newark is', airport_transaction['Fare_amount'].mean(), 'USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "-FOyR4BERX_Y",
    "outputId": "46688273-a3ec-4042-c8e1-7fea132250e1"
   },
   "outputs": [],
   "source": [
    "#let's see the number of transactions grouped by the hour\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(airport_transaction['Fare_amount'].groupby(df['hour']).count())\n",
    "plt.title('Number Of Trips Grouped By Hour Of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number Of Trips')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(airport_transaction['Fare_amount'].groupby(df['hour']).mean())\n",
    "plt.title('Fare Amount Grouped By Hour Of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fare Amount In USD')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(airport_transaction['Fare_amount'].groupby(df['hour']).mean())\n",
    "plt.title('Average Trip Distance Grouped By Hour Of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fare Amount In USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IeuE7XDS9lj"
   },
   "source": [
    "We can see that the peak hour of the day in the airport departures is around 3PM, Also the highest average of trip distance is at 3PM. Also the highest Average fare is at 3PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laY0m3LvmgzX"
   },
   "source": [
    "#Question 4\n",
    "Question 4\n",
    "\n",
    "•             Build a derived variable for tip as a percentage of the total fare.\n",
    "\n",
    "•             Build a predictive model for tip as a percentage of the total fare. Use as much of the data as you like (or all of it). Provide an estimate of performance using an appropriate sample, and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7b2fvinuFPz"
   },
   "source": [
    "adding \n",
    "* Tip Percentage from the total fair\n",
    "* Trip day of the week\n",
    "* Trip Duration In Minutes\n",
    "* Trip Speed\n",
    "* Trip pick up hour\n",
    "* Trip day of the month\n",
    "* Trip avergae speed\n",
    "* Trip Week of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZnMRKFprmrdt",
    "outputId": "00e6ac72-6d7d-4d0a-a974-ebcf868e3b99"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#this takes around 7 mis to run\n",
    "#get tip percentage\n",
    "#calculates the tip percentage from the fare\n",
    "df['tip_perc'] = (df['Tip_amount']/df['Fare_amount'])*100 \n",
    "\n",
    "#there are wierdly 400 values on nan after doing this, so i will impute them by the median\n",
    "df['tip_perc'] = df.tip_perc.apply(lambda x: df['tip_perc'].mean() if math.isnan(x)==True else x)\n",
    "\n",
    "#get day of the week\n",
    "df['day'] = pd.to_numeric(df['day'])\n",
    "\n",
    "#find which day of the week it is, Mon = 1, Sun = 7\n",
    "df['day_of_the_week']= df.day.apply(lambda x: datetime.date(2015,9,x).isoweekday())\n",
    "\n",
    "\n",
    "#get trip duration in minutes\n",
    "#this is not very readable but essentialy what i am doing is i am taking the difference in time between\n",
    "#dropoff and pickup and then taking the total seconds of the trip and converting to minutes\n",
    "df['trip_min'] = ((df.Lpep_dropoff_datetime.apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))-df.lpep_pickup_datetime.apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))).apply(lambda y:y.seconds/60.))\n",
    "\n",
    "#this fixes the trip minute = 1 instead of zero so we can calculate the speed\n",
    "df['trip_min']=df.trip_min.apply(lambda x: df['trip_min'].median() if x == 0 else x)\n",
    "\n",
    "\n",
    "#get trip duration in minutes from seconds\n",
    "df['trip_hr'] = df['trip_min']/60\n",
    "\n",
    "#change to numeric\n",
    "df['trip_hr'] = pd.to_numeric(df['trip_hr'])\n",
    "\n",
    "#assumption is a trip more than 7 hours is data eror and replaced by median!!this takes 5 mins to run by itself!!\n",
    "df['trip_hr']=df.trip_hr.apply(lambda x: df['trip_hr'].median() if x > 7  else x)\n",
    "\n",
    "#get the speed of the taxi miles/hr\n",
    "df['speed'] = df['Trip_distance']/df['trip_hr']\n",
    "\n",
    "#remove trips with speed > 50 mph, we will consider it as data error(around 4000 observation)\n",
    "#a better way is to discretize the trip distance and then group by the trip distance and imoute by the median within the \n",
    "#given group, but due to time constraint i will just remove them\n",
    "df = df[df['speed'] < 50]\n",
    "\n",
    "#this will be used later in question 5\n",
    "df['week_of_month'] = df.day.apply(lambda x: datetime.date(2015,9,x).isocalendar()[1]-35) #find which week of the mont 1 is week1 2 is week2 ...\n",
    "\n",
    "#create a new column named time_boundries, i have an intuition the time of the ride is important factor of giving the tip or no\n",
    "#discretice the pick up time to morning rush, lunch rush, evening rush, and others\n",
    "#change to numeric\n",
    "df['hour'] = pd.to_numeric(df['hour'])\n",
    "mapDict={}\n",
    "for i in range (0,24):\n",
    "  if i>4 and i<10: #5 am tp 9 pm\n",
    "    mapDict[i]=\"morning rush\"\n",
    "  elif i>10 and i<3: #11am to 2pm\n",
    "    mapDict[i]=\"lunch rush\"\n",
    "  elif i>15 and i<20: #4pm to 7pm\n",
    "    mapDict[i]=\"evening rush\"\n",
    "  else:\n",
    "    mapDict[i]=\"others\"\n",
    "df['timeBoundary']=df['hour'].map(mapDict)\n",
    "#need to test if this is working\n",
    "#make new column to indicate if it's a weekday or weekend\n",
    "map_dict={}\n",
    "for i in range (1,7):\n",
    "  if i<6:\n",
    "    map_dict[i]=\"week day\"\n",
    "  else:\n",
    "    mapDict[i]=\"week end\"\n",
    "df['is_weekday']=df['day_of_the_week'].map(mapDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "qU48vyZNBxbb",
    "outputId": "79dd226a-ae18-4d53-ba32-97ea981beb2d"
   },
   "outputs": [],
   "source": [
    "#let's see the number of transactions grouped by the hour\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(df['Passenger_count'].groupby(df['day_of_the_week']).mean())\n",
    "plt.title('Average Number Of Passengers Grouped  By Day Of The Week')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Passengers')\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(df['Trip_distance'].groupby(df['day_of_the_week']).mean())\n",
    "plt.title('Average Trip Distance Grouped  By Day Of The Week')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Distance')\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(df['Fare_amount'].groupby(df['day_of_the_week']).mean())\n",
    "plt.title('Fare Amount Grouped By Day Of The Week')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fare Amount')\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(df['Tip_amount'].groupby(df['day_of_the_week']).mean())\n",
    "plt.title('Tip Amount Grouped By Day Of The Week')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Tip Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9MyPSICETkz"
   },
   "source": [
    "* From this we can see that the peak of average tip amount is in the weekend\n",
    "* Also the average number of passengers peaks in the weekend\n",
    "* Also the Average trip distance peaks in the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNdRFH9-8Lui"
   },
   "outputs": [],
   "source": [
    "#this is the code for getting the zip code from google api but didn't work because google limits the number of calls\n",
    "\n",
    "#%%time\n",
    "#for i in range(len(df)):\n",
    "#  address=[len(df)]\n",
    "#  location = geolocator.reverse([df['Dropoff_latitude'][i],df['Dropoff_longitude'][i]])\n",
    "#  if 'postcode' in location.raw['address']:\n",
    "#    address.append(location.raw['address']['postcode'])\n",
    "#  else:\n",
    "#    address.append(\"NA\")\n",
    "  #df['zip_code'][i] = address[i].raw['address']['postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1655
    },
    "colab_type": "code",
    "id": "YU4yObEJjCRV",
    "outputId": "868c109c-c402-49bc-d570-9cd6841d983f"
   },
   "outputs": [],
   "source": [
    "# generated using help from the following tutorial:\n",
    "# http://www.datadependence.com/2016/06/creating-map-visualisations-in-python/\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import MatplotlibDeprecationWarning\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', MatplotlibDeprecationWarning)\n",
    "%matplotlib inline \n",
    "plt.subplot(2,1,1)\n",
    "westlimit=-74.2591\n",
    "southlimit=40.4774\n",
    "eastlimit=-73.7002\n",
    "northlimit=40.9162\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "m = Basemap(resolution='c', projection='merc',\n",
    "            lat_0=(northlimit-southlimit)/2, \n",
    "            lon_0=(westlimit-eastlimit)/2,\n",
    "            llcrnrlon=westlimit, llcrnrlat=southlimit, \n",
    "            urcrnrlon=eastlimit, urcrnrlat=northlimit)\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='#FFFFFF', lake_color='#FFFFFF')\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "m.drawmapboundary(fill_color='#FFFFFF')\n",
    "long = df.Dropoff_longitude.tolist()\n",
    "lat = df.Dropoff_latitude.tolist()\n",
    "x, y = m(long, lat)\n",
    "m.plot(x, y, 'o', markersize=4, color='#039BE5', alpha=0.7)\n",
    "plt.title('Drop Off Locations', fontsize=18)\n",
    "plt.show()\n",
    "plt.subplot(2,1,2)\n",
    "westlimit=-74.2591\n",
    "southlimit=40.4774\n",
    "eastlimit=-73.7002\n",
    "northlimit=40.9162\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "m = Basemap(resolution='c', projection='merc',\n",
    "            lat_0=(northlimit-southlimit)/2, \n",
    "            lon_0=(westlimit-eastlimit)/2,\n",
    "            llcrnrlon=westlimit, llcrnrlat=southlimit, \n",
    "            urcrnrlon=eastlimit, urcrnrlat=northlimit)\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='#FFFFFF', lake_color='#FFFFFF')\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "m.drawmapboundary(fill_color='#FFFFFF')\n",
    "long = df.Pickup_longitude.tolist()\n",
    "lat = df.Pickup_latitude.tolist()\n",
    "x, y = m(long, lat)\n",
    "m.plot(x, y, 'o', markersize=4, color='#039BE5', alpha=0.7)\n",
    "plt.title('Pick Up Locations', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VtgV5pbFPOi"
   },
   "source": [
    "Not a pretty map but good enough to tell us some informatio\n",
    "* We can see that the pick up location is concentrated in 2 areas\n",
    "* The drop off locations are more scattered\n",
    "\n",
    "I am not expert with NY maps, but if we have more domain knowledge on which 2 areas are those, we would be able to understand more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "UBn1wdguGQpp",
    "outputId": "58e50761-aa98-46ca-f5e1-d458307b23d0"
   },
   "outputs": [],
   "source": [
    "#distribution of the response variable\n",
    "#let's take the zero's out and check again\n",
    "sns.distplot(df['tip_perc'])\n",
    "plt.xlim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "AeGQ-Z0g4-fE",
    "outputId": "0b51c200-de3f-44a8-dee3-d00402d1bbfd"
   },
   "outputs": [],
   "source": [
    "print('Number of fares with no tips is',((df['tip_perc'][df['tip_perc']==0]).count()) )\n",
    "print('Number of trips with tips is', ((df['tip_perc']).count()))\n",
    "print('Percentage of trips with no fairs is',(100*884810)/1484696,'%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "AlZGmnkN4514",
    "outputId": "fab85619-58e1-40c8-e6a1-11c9e433e4a5"
   },
   "outputs": [],
   "source": [
    "trips_with_tip = df[df['tip_perc'] > 0]\n",
    "sns.distplot(trips_with_tip['tip_perc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tipw2t8UZsuW"
   },
   "source": [
    "Looks way better now, looks like there is a peak around 20% tips, is there a standard tip for taxi's like reseraunts? that would be a good question to investigate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "axvuGVTpZlwa",
    "outputId": "01e34f85-e98f-4350-e706-6b5c9555d35a"
   },
   "outputs": [],
   "source": [
    "plt.hist(trips_with_tip['tip_perc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaTC75Q27AeL"
   },
   "source": [
    "##Data Quality Verification\n",
    "\n",
    "We already did some data cleaning, but we didn't look at the missing values yet, so we eill take a deep dive in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "E_r7W9JgJbYx",
    "outputId": "4f9ba536-a470-4056-8e2e-69bd34ecbead"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "jz_Ru0vF7Del",
    "outputId": "f3223212-8b0e-40bd-8054-06d576011750"
   },
   "outputs": [],
   "source": [
    "import missingno as mn\n",
    "mn.matrix(df.sort_values(by=['Trip_distance','RateCodeID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "6SK0BFnV8zRd",
    "outputId": "bd0bf0a4-0bbf-4c3f-cf6a-6c57eda246ad"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWZicwav9c6F"
   },
   "source": [
    "looks like all the Ehail_fee is missing, so i am going to drop that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PRYTgBH9NCY"
   },
   "outputs": [],
   "source": [
    "df.drop('Ehail_fee', axis=1, inplace=True)\n",
    "#df['Trip_type '] = df['Trip_type '].apply(lambda x: df['Trip_type '].mode() if math.isnan(x) == True else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW9pKmnaajuh"
   },
   "source": [
    "#Need to do the Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97Mkrj0_3wF2"
   },
   "source": [
    "We have a huge problem here, since the response variable is continous and 60% of the data is zero's\n",
    "If you fit a regression model with least square it would be heavily biased towards 0, as most of our data has tip_perc = 0.\n",
    "\n",
    "An idea is to try to do a classification problem where we predict that if the passenger is going to pay a tip fare or no\n",
    "\n",
    "and if the model predicts he is going to tip, we predict what's the tip percentage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqMq66MC9Mgc"
   },
   "source": [
    "#Preparing the variables for modeling\n",
    "* Drop the Vendor ID because it's nominal variable\n",
    "\n",
    "* Drop the lpep_pickup_datetime'/pep_dropoff_datetime since we already took the information of them into 4 different columns\n",
    "\n",
    "* Drop Latitude/Longitude since the model can't use them(i would have loved to have the zip codes)\n",
    "\n",
    "* Drop the Extra column because 30% of the data is negative and this is supposed to be extra charges\n",
    "\n",
    "* Drop the MTA_tax since it's a fixed charge that's triggered when the meter turns on\n",
    "\n",
    "* Drop improvment surcharge since it's afixed rate that just began in 2015, and if we want to use previous years data for training, we won't have this column\n",
    "\n",
    "* Drop the Tip_amount, since our response variable is derieved from it and this will be cheating on the model\n",
    "\n",
    "* Drop trip_hour since we already have trip_min\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "IzQoB1QNCo21",
    "outputId": "fdad69b4-be5d-4a4a-edaa-14ce55a6f61d"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVRMcGzMBecA"
   },
   "outputs": [],
   "source": [
    "df_model = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1hYjNqnVBkc3",
    "outputId": "07c7b74e-25d3-4a5a-87dd-f9cb23ef1f41"
   },
   "outputs": [],
   "source": [
    "df_model = df_model.drop(['VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime', 'Store_and_fwd_flag', 'RateCodeID', 'Pickup_longitude', 'Pickup_latitude','Dropoff_longitude', 'Dropoff_latitude',\n",
    "        'Tip_amount', 'Extra', 'MTA_tax', 'improvement_surcharge', 'Tip_amount', 'trip_hr'], axis = 1)\n",
    "df_model.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyYYX4HdOBHo"
   },
   "source": [
    "##Build a simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGi1zd-sO8Yh"
   },
   "source": [
    "change the following to object type: Passenger_count, Payment_type, Trip_type, Hour ,day , day_of_the_week, week_of_month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "u4cd-CQeN-53",
    "outputId": "e79d7fd8-606f-43f1-db85-3c0faf595302"
   },
   "outputs": [],
   "source": [
    "df_model['Passenger_count'] = df_model['Passenger_count'].astype(object)\n",
    "df_model['Payment_type'] = df_model['Payment_type'].astype(object)\n",
    "df_model['Trip_type '] = df_model['Trip_type '].astype(object)\n",
    "df_model['hour'] = df_model['hour'].astype(object)\n",
    "df_model['day'] = df_model['day'].astype(object)\n",
    "df_model['day_of_the_week'] = df_model['day_of_the_week'].astype(object)\n",
    "df_model['week_of_month'] = df_model['week_of_month'].astype(object)\n",
    "print(df_model.dtypes)\n",
    "print(df_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAhScHO9S8O_"
   },
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nas3nJteGxEc",
    "outputId": "eafe19c1-a546-4551-d978-23d8dfbfd890"
   },
   "outputs": [],
   "source": [
    "df_dummy = pd.get_dummies(df_model, columns=['Passenger_count', 'Payment_type', 'Trip_type ', 'hour', 'day', 'day_of_the_week', 'week_of_month', 'timeBoundary', 'is_weekday'])\n",
    "df_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN_1vpUFYcX4"
   },
   "source": [
    "Normalize continous features using sklear preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVbwIMCqVTTP"
   },
   "outputs": [],
   "source": [
    "df_normalized = df_dummy\n",
    "df_normalized['Trip_distance'] = preprocessing.normalize(df_normalized['Trip_distance'].reshape(-1,1))\n",
    "df_normalized['Fare_amount'] = preprocessing.normalize(df_normalized['Fare_amount'].reshape(-1,1))\n",
    "df_normalized['Tolls_amount'] = preprocessing.normalize(df_normalized['Tolls_amount'].reshape(-1,1))\n",
    "df_normalized['trip_min'] = preprocessing.normalize(df_normalized['trip_min'].reshape(-1,1))\n",
    "df_normalized['speed'] = preprocessing.normalize(df_normalized['speed'].reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pil940a6ZFSV",
    "outputId": "00879c52-f425-4b49-b10a-fbfd8e0d621b"
   },
   "outputs": [],
   "source": [
    "y = df_normalized['tip_perc'] #this now has the response variable\n",
    "df_normalized = df_normalized.drop(['tip_perc'], axis = 1) #this has the features\n",
    "print(y.shape)\n",
    "print(df_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DnB4wWtMYi2-",
    "outputId": "c91236a9-fa7c-4f0f-c826-ee694b83f46a"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_normalized, y, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOHGRHGpbAIj"
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "LrS03WEObFKx",
    "outputId": "41ed8093-c33b-4de7-ba3b-ba7f8ea4a0b7"
   },
   "outputs": [],
   "source": [
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('True Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iud-jOb-h_"
   },
   "source": [
    "If this is the best model we would have all the points lying on a 45 degree line from the origin, but we can see that the model is biased to zero's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wMgq6Q3TbUCp",
    "outputId": "9543941a-7467-4f2d-a662-a7b6e5aa0089"
   },
   "outputs": [],
   "source": [
    "print('Our simple linear regression model score is:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQNK4lcZcV-e"
   },
   "source": [
    "Now let's try using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5VA9iqqDgUF_",
    "outputId": "23949906-99f7-4adc-cac8-ec2f67110f82"
   },
   "outputs": [],
   "source": [
    "X = np.array(df_normalized) # create an array\n",
    "y = np.array(y) # Create another array\n",
    "kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf) \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "UotNZuOZcZSs",
    "outputId": "372ec304-60ad-46d5-e178-5ca5451ce0d1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print ('Cross-validated scores:', scores)\n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, X, y, cv=5)\n",
    "plt.scatter(y, predictions)\n",
    "accuracy = metrics.r2_score(y, predictions)\n",
    "print ('Cross-Predicted Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqtIU6dDjPZl"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "DM_train = xgb.DMatrix(data = X_train, \n",
    "                       label = y_train)  \n",
    "DM_test =  xgb.DMatrix(data = X_test,\n",
    "                       label = y_test)\n",
    "gbm_param_grid = {\n",
    "     'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "     'n_estimators':[100, 200],\n",
    "     'max_depth': [10, 15, 20, 25]\n",
    "}\n",
    "gbm = xgb.XGBRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AlM5hayojx5j",
    "outputId": "30cb0dc3-acdc-456e-85ff-59d3b764505c"
   },
   "outputs": [],
   "source": [
    "grid_mse = GridSearchCV(estimator = gbm, param_grid = gbm_param_grid, scoring = 'neg_mean_squared_error', cv = 5, verbose = 1)\n",
    "grid_mse.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "pred = grid_mse.predict(X_test)\n",
    "print(\"Root mean square error for test dataset: {}\".format(np.round(np.sqrt(mean_squared_error(y_test, pred)), 2)))\n",
    "test = pd.DataFrame({\"prediction\": pred, \"observed\": y_test.flatten()})\n",
    "lowess = sm.nonparametric.lowess\n",
    "z = lowess(pred.flatten(), y_test.flatten())\n",
    "test.plot(figsize = [14,8],\n",
    "          x =\"prediction\", y = \"observed\", kind = \"scatter\", color = 'darkred')\n",
    "plt.title(\"Extreme Gradient Boosting: Prediction Vs Test Data\", fontsize = 18, color = \"darkgreen\")\n",
    "plt.xlabel(\"Predicted Power Output\", fontsize = 18) \n",
    "plt.ylabel(\"Observed Power Output\", fontsize = 18)\n",
    "plt.plot(z[:,0], z[:,1], color = \"blue\", lw= 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWe4SvbKao6U"
   },
   "source": [
    "##Question 5\n",
    "\n",
    "Choose only one of these options to answer for Question 5. There is no preference as to which one you choose. Please select the question that you feel your particular skills and/or expertise are best suited to. If you answer more than one, only the first will be scored.\n",
    "\n",
    "•             Option A: Distributions\n",
    "\n",
    "•             Build a derived variable representing the average speed over the course of a trip.\n",
    "\n",
    "•             Can you perform a test to determine if the average trip speeds are materially the same in all weeks of September? If you decide they are not the same, can you form a hypothesis regarding why they differ?\n",
    "\n",
    "•             Can you build up a hypothesis of average trip speed as a function of time of day?\n",
    "\n",
    "•             Option B: Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "SqLdT2ndYkDp",
    "outputId": "9af82270-90df-4598-e4b4-db47b878aa8f"
   },
   "outputs": [],
   "source": [
    "x = df.groupby(df['week_of_month'])\n",
    "x[['speed']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "Mo2l4YgFceEs",
    "outputId": "b40dc1ec-295d-4c61-cef9-99fbd73d622d"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='week_of_month', y='speed', data =df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "WT3inugOdkvJ",
    "outputId": "eb6c9d50-de8e-4284-efca-ccb14371ec3b"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1HwVeSNdYmA"
   },
   "outputs": [],
   "source": [
    "#plt.subplots(figsize=(20, 5))\n",
    "#args = {'x':\"week_of_month\", 'y':\"speed\", 'data':df}\n",
    "#funcs_to_plot = [sns.boxplot,sns.violinplot,sns.swarmplot]\n",
    "#for i, plot_func in enumerate(funcs_to_plot):\n",
    "#    plt.subplot(1,3,i+1)\n",
    "#    plot_func(**args)\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Br5I-WZpbpZD"
   },
   "outputs": [],
   "source": [
    "#df.boxplot(column = 'speed',by='week_of_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "PcjRq5zMJVo4",
    "outputId": "8e7f76e3-420f-458e-9f2b-8e63d5198ec8"
   },
   "outputs": [],
   "source": [
    "df_week_grouped = df.groupby(df['week_of_month'])[[ 'speed']]\n",
    "plt.boxplot(df_week_grouped.mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "SBzFwFAZYOxt",
    "outputId": "ea72c944-0aa4-445f-baae-6ed4898f9289"
   },
   "outputs": [],
   "source": [
    ")df[['week_of_month','speed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpuwuW8P2aD0"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "week_1 = \n",
    "\n",
    "stats.f_oneway(week_1,week_2, week_3,week_4, week_5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "code_challenge.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
